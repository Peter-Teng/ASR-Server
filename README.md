# ASR-Server
A simple ASR server based on Python (3.9), using FastAPI (web server), fsmn-vad, sensevoice-small (speech transcription) and ERes2Net-Large (speaker verification). It can register speaker, then transcribes audio while identify the specific speaker.

---
## Quick Start
``` shell
git clone git@github.com:Peter-Teng/ASR-Server.git
cd ASR-Server
# Using Anaconda
# skip to pip install if anaconda is not available
conda create -n server python=3.9
conda activate server
# Installing requirements
pip install -r requirements.txt
```
Downloading the models is required when starting the application for the first time (this may take a few minutes). Then the web service should be able to work offline.


The default host is *localhost*, and the server will serve at port *8000* (host and port could be changed).
``` shell
python app.py --download
```

## Usage
The server contains serveral APIs. **"/transcribe"** is for transcribing a certain audio file (only support .wav currently). **"/register"** is for registing a speaker to the server in order to be recognized.


Here is a basic example (on Windows), the example audio files could be found at the folder *example*.
```shell
# /register
curl --location --request POST "localhost:8000/v1/speaker/register" ^
--header "Content-Type: application/json" ^
--data-raw "{    \"audioPath\": \"{your_path}/examples/staff.wav\",    \"name\": \"staff\"}"

# /transcribe
curl --location --request POST "localhost:8000/v1/transcribe/do" ^
--header "Content-Type: application/json" ^
--data-raw "{    \"path\": \"{your_path}/examples/customer_service.wav\"}"
```
The result would be:
```json
{
    "code": "0",
    "data": {
        "transcribe_results": [
            {
                "content": "Some Sentences",
                "speaker": "Somebody"
            },
            ...
        ]
    },
    "msg": "success"
}
```
For more usages, please refer to the Swagger docs that automatically generated by FastAPI.


---

## Reference

```
@inproceedings{gao2023funasr,
  author={Zhifu Gao and Zerui Li and Jiaming Wang and Haoneng Luo and Xian Shi and Mengzhe Chen and Yabin Li and Lingyun Zuo and Zhihao Du and Zhangyu Xiao and Shiliang Zhang},
  title={FunASR: A Fundamental End-to-End Speech Recognition Toolkit},
  year={2023},
  booktitle={INTERSPEECH},
}
@article{an2024funaudiollm,
  title={Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms},
  author={An, Keyu and Chen, Qian and Deng, Chong and Du, Zhihao and Gao, Changfeng and Gao, Zhifu and Gu, Yue and He, Ting and Hu, Hangrui and Hu, Kai and others},
  journal={arXiv preprint arXiv:2407.04051},
  year={2024}
}
@inproceedings{zhang2018deep,
  title={Deep-FSMN for large vocabulary continuous speech recognition},
  author={Zhang, Shiliang and Lei, Ming and Yan, Zhijie and Dai, Lirong},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5869--5873},
  year={2018},
  organization={IEEE}
}
@inproceedings{chen2023enhanced,
  title={An Enhanced Res2Net with Local and Global Feature Fusion for Speaker Verification},
  author={Chen, Yafeng and Zheng, Siqi and Wang, Hui and Cheng, Luyao and Chen, Qian and Qi, Jiajun},
  booktitle={INTERSPEECH},
  year={2023}
}
@inproceedings{zheng20233d,
  title={3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement},
  author={Siqi Zheng, Luyao Cheng, Yafeng Chen, Hui Wang and Qian Chen},
  url={https://arxiv.org/pdf/2306.15354},
  year={2023}
}
@article{chen20243d,
  title={3D-Speaker-Toolkit: An Open Source Toolkit for Multi-modal Speaker Verification and Diarization},
  author={Chen, Yafeng and Zheng, Siqi and Wang, Hui and Cheng, Luyao and others},
  url={https://arxiv.org/pdf/2403.19971},
  year={2024}
}

```
